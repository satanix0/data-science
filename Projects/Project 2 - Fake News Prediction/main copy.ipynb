{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection Project\n",
    "## Using Logistic Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn. feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20705</th>\n",
       "      <td>Tony Romo’s Star Is Eclipsed by Another Feel-G...</td>\n",
       "      <td>Juliet Macur</td>\n",
       "      <td>FRISCO, Tex.  —   The Dallas Cowboys’ new head...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>Mosul Suicide Bomber Was British, Says Islamic...</td>\n",
       "      <td>Breitbart London</td>\n",
       "      <td>BAGHDAD (AFP)  —   The Islamic State group on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>No, Mexico Doesn’t Have A Wall On Its Southern...</td>\n",
       "      <td>Allan Wall</td>\n",
       "      <td>X Dear Reader! VDARE.com isn’t just a website....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>North Korea’s Nuclear Blasts Keep Getting Stro...</td>\n",
       "      <td>Michael Forsythe</td>\n",
       "      <td>North Korea said it conducted its fifth underg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14010</th>\n",
       "      <td>State Department Informants Tip Clinton Campai...</td>\n",
       "      <td>Terresa Monroe-Hamilton</td>\n",
       "      <td>1 comment \\nGee, the State Department and Hill...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "20705  Tony Romo’s Star Is Eclipsed by Another Feel-G...   \n",
       "7239   Mosul Suicide Bomber Was British, Says Islamic...   \n",
       "9008   No, Mexico Doesn’t Have A Wall On Its Southern...   \n",
       "1298   North Korea’s Nuclear Blasts Keep Getting Stro...   \n",
       "14010  State Department Informants Tip Clinton Campai...   \n",
       "\n",
       "                        author  \\\n",
       "20705             Juliet Macur   \n",
       "7239          Breitbart London   \n",
       "9008                Allan Wall   \n",
       "1298          Michael Forsythe   \n",
       "14010  Terresa Monroe-Hamilton   \n",
       "\n",
       "                                                    text  label  \n",
       "20705  FRISCO, Tex.  —   The Dallas Cowboys’ new head...      0  \n",
       "7239   BAGHDAD (AFP)  —   The Islamic State group on ...      0  \n",
       "9008   X Dear Reader! VDARE.com isn’t just a website....      1  \n",
       "1298   North Korea said it conducted its fifth underg...      0  \n",
       "14010  1 comment \\nGee, the State Department and Hill...      1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(columns='id')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   20242 non-null  object\n",
      " 1   author  18843 non-null  object\n",
      " 2   text    20761 non-null  object\n",
      " 3   label   20800 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 650.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna('')\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate input and target\n",
    "X = train.drop(columns='label')\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing a complex word to its Root i.e. most simplest word <br>\n",
    "example: actor, actress, acting --> act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "\n",
    "def stemming(content):\n",
    "    \"\"\"\n",
    "    Perform stemming on the given content.\n",
    "    Args:\n",
    "        content (str): The input text to be stemmed.\n",
    "    Returns:\n",
    "        str: The stemmed version of the input text.\n",
    "    \"\"\"\n",
    "    # Initialize the stemmer and preprocess the content\n",
    "    stemmer = PorterStemmer()\n",
    "    # Remove non-alphabetic characters (numerics and special characters) and convert to lowercase\n",
    "    clean_content = re.sub(r'[^a-zA-Z]', ' ', content).lower()\n",
    "\n",
    "    # Split into words and filter out stopwords\n",
    "    filtered_words = [\n",
    "        stemmer.stem(word)\n",
    "        for word in clean_content.split()\n",
    "        if word not in stopwords.words('english')\n",
    "    ]\n",
    "\n",
    "    # Join the stemmed words back into a single string sepreated by empty spaces\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello dark old friend'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the 'stemming' function on the example string\n",
    "test_stemming_func = \"Hello Darkness! my old friend 123\"\n",
    "stemming(test_stemming_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hous dem aid even see comey letter jason chaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn hillari clinton big woman campu breitbar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truth might get fire consortiumnew com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>civilian kill singl us airstrik identifi jessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jail fiction unpublish stori wom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  hous dem aid even see comey letter jason chaff...\n",
       "1  flynn hillari clinton big woman campu breitbar...\n",
       "2             truth might get fire consortiumnew com\n",
       "3  civilian kill singl us airstrik identifi jessi...\n",
       "4  iranian woman jail fiction unpublish stori wom..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_title_author(df):\n",
    "    \"\"\"Combines 'title' and 'author' columns into a single 'content' column.\"\"\"\n",
    "    df['content'] = df['title'] + ' - by ' + df['author']\n",
    "    return df[['content']]\n",
    "\n",
    "\n",
    "def content_stemming(df):\n",
    "    \"\"\"Applies stemming to the 'content' column in the DataFrame.\"\"\"\n",
    "    df.loc[:, 'content'] = df['content'].apply(\n",
    "        stemming)  # Apply stemming to the content column\n",
    "    return df[['content']]\n",
    "\n",
    "\n",
    "# post-preprocessing : JUST FOR DEMONSTARATION PURPOSES\n",
    "content_stemming(combine_title_author(X)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "- As the values are still in text format, we need to convert them in the numeric frm in order to work with ML models\n",
    "- Converting Textual Data to Numeric is called ***Vectorization***\n",
    "- We will use `TfidfVectorizer()`, which  converts a raw count matrix (produced by CountVectorizer) into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix. It applies weighting to each term based on its frequency within the document and across the entire corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('combine_content', FunctionTransformer(lambda df: combine_title_author(\n",
    "            df), validate=False)),  # Combine title and author\n",
    "        ('content_stemming', FunctionTransformer(lambda df: content_stemming(\n",
    "            df), validate=False)),  # stemming the strings in each content\n",
    "        # Apply TFIDF Vectorizer to 'content' column\n",
    "        ('tf-ifd',\n",
    "         ColumnTransformer(transformers=[('tfidf', TfidfVectorizer(), 'content')])),\n",
    "        ('logreg', LogisticRegression(max_iter=200))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14560, 4)\n",
      "(6240, 4)\n",
      "(14560,)\n",
      "(6240,)\n"
     ]
    }
   ],
   "source": [
    "# stratify=y make sure that training and validation sets have equal proportion of the different target labels.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred_val = final_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746256895193065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      3116\n",
      "           1       0.96      0.99      0.97      3124\n",
      "\n",
      "    accuracy                           0.97      6240\n",
      "   macro avg       0.97      0.97      0.97      6240\n",
      "weighted avg       0.97      0.97      0.97      6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "print(f1_score(y_val, y_pred_val))\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "No Need to optimize hyperparameters as the model is already performing really well, The model is highly accurate (97%) and performs well on both classes, as indicated by the high precision, recall, and F1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>22821</td>\n",
       "      <td>SU 35 Vs F 35 Unbiased Detailed Comparison: To...</td>\n",
       "      <td>wmw_admin</td>\n",
       "      <td>Behind the headlines - conspiracies, cover-ups...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>23513</td>\n",
       "      <td>Torch-Wielding Mob Of Democrat Villagers Grab ...</td>\n",
       "      <td>Daisy Luther</td>\n",
       "      <td>in: Special Interests , US News Do you remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>24165</td>\n",
       "      <td>Pat Cleveland: Early Supermodel and Author Wit...</td>\n",
       "      <td>Guy Trebay</td>\n",
       "      <td>WILLINGBORO, N. J.  —   The peacocks were root...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>24331</td>\n",
       "      <td>#LoudonClear: Tonight’s Special Guest Tim Burton</td>\n",
       "      <td>Trevor Loudon</td>\n",
       "      <td>We are Gulag Bound / *Resisters' Log* / #Lou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>24050</td>\n",
       "      <td>Palestine Considers Suing Israel in Internatio...</td>\n",
       "      <td>James M. Dorsey</td>\n",
       "      <td>2 Shares\\n2 0 0 0\\nThe Palestine Football Asso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "2021  22821  SU 35 Vs F 35 Unbiased Detailed Comparison: To...   \n",
       "2713  23513  Torch-Wielding Mob Of Democrat Villagers Grab ...   \n",
       "3365  24165  Pat Cleveland: Early Supermodel and Author Wit...   \n",
       "3531  24331   #LoudonClear: Tonight’s Special Guest Tim Burton   \n",
       "3250  24050  Palestine Considers Suing Israel in Internatio...   \n",
       "\n",
       "               author                                               text  \n",
       "2021        wmw_admin  Behind the headlines - conspiracies, cover-ups...  \n",
       "2713     Daisy Luther  in: Special Interests , US News Do you remembe...  \n",
       "3365       Guy Trebay  WILLINGBORO, N. J.  —   The peacocks were root...  \n",
       "3531    Trevor Loudon    We are Gulag Bound / *Resisters' Log* / #Lou...  \n",
       "3250  James M. Dorsey  2 Shares\\n2 0 0 0\\nThe Palestine Football Asso...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title     122\n",
       "author    503\n",
       "text        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.fillna('')\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m final_y_pred \u001b[38;5;241m=\u001b[39m final_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\piyus\\anaconda3\\envs\\datascience\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\piyus\\anaconda3\\envs\\datascience\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\piyus\\anaconda3\\envs\\datascience\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\piyus\\anaconda3\\envs\\datascience\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['label'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X_test = test.drop(columns='label')\n",
    "y_test = test['label']\n",
    "\n",
    "final_y_pred = final_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, final_y_pred))\n",
    "print(classification_report(y_test, final_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
